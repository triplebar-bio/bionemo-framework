# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-Apache2
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any, Callable, Dict, Literal, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from megatron.core import parallel_state, tensor_parallel
from megatron.core.utils import get_batch_on_this_cp_rank
from nemo.collections.llm.gpt.model.hyena import HyenaConfig
from nemo.collections.llm.gpt.model.hyena import HyenaModel as ForwardHyenaModel
from nemo.collections.llm.gpt.model.megatron.hyena.hyena_model import HyenaModel as CoreHyenaModel
from nemo.collections.llm.gpt.model.megatron.hyena.hyena_utils import (
    make_upper_case,
    reweighted_cross_entropy,
)
from nemo.lightning.io.mixin import IOMixin
from nemo.utils import logging

from ..loss.rnaseq_head import BorzoiLoss, HuberLoss, HybridLoss, PoissonWithDistributionLoss
from .debug import debug_heads


class BioSignalHead(nn.Module):
    """Simplified head with batch normalization for stable training.

    Uses batch norm instead of layer norm to reduce internal covariate shift
    while preserving magnitude information critical for expression prediction.
    """

    def __init__(
        self,
        hidden_size: int,
        intermediate_size: int = 256,
        num_layers: int = 5,
        dropout: float = 0.1,
        use_batch_norm: bool = True,  # New parameter
        config: Optional[HyenaConfig] = None,
        init_method: Optional[Callable] = None,
        input_is_parallel: bool = False,
    ):
        """Initializes the RNA-seq prediction head.

        Args:
            hidden_size: Size of GLM embeddings
            intermediate_size: Hidden dimension
            num_layers: Number of layers (1-2 recommended)
            dropout: Dropout rate
            use_batch_norm: Whether to use batch normalization (recommended)
            config: Model config for tensor parallel layers
            init_method: Weight initialization method
            input_is_parallel: Whether input is in parallel format
        """
        super().__init__()

        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.num_layers = num_layers
        self.use_batch_norm = use_batch_norm

        # Build MLP layers
        self.layers = nn.ModuleList()
        self.batch_norms = nn.ModuleList() if use_batch_norm else None

        if num_layers == 1:
            # Direct projection: hidden_size -> 1
            self.layers.append(
                tensor_parallel.ColumnParallelLinear(
                    hidden_size,
                    1,
                    config=config,  # type: ignore
                    init_method=init_method,  # type: ignore
                    bias=True,
                    gather_output=True,
                    skip_bias_add=False,
                )
            )
            # No batch norm for single layer (output layer)
        else:
            # First layer: hidden_size -> intermediate_size
            self.layers.append(
                tensor_parallel.ColumnParallelLinear(
                    hidden_size,
                    intermediate_size,
                    config=config,  # type: ignore
                    init_method=init_method,  # type: ignore
                    bias=True,  # Batch norm has its own bias, but keeping for consistency
                    gather_output=False,
                    skip_bias_add=False,
                )
            )
            if isinstance(self.batch_norms, nn.ModuleList):
                self.batch_norms.append(nn.BatchNorm1d(intermediate_size))

            # Middle layers: intermediate_size -> intermediate_size
            for _ in range(num_layers - 2):
                self.layers.append(
                    tensor_parallel.RowParallelLinear(
                        intermediate_size,
                        intermediate_size,
                        config=config,  # type: ignore
                        init_method=init_method,  # type: ignore
                        bias=True,
                        input_is_parallel=input_is_parallel,
                        skip_bias_add=False,
                    )
                )
                if isinstance(self.batch_norms, nn.ModuleList):
                    self.batch_norms.append(nn.BatchNorm1d(intermediate_size))

            # Final layer: intermediate_size
            self.layers.append(
                tensor_parallel.RowParallelLinear(
                    intermediate_size,
                    1,
                    config=config,  # type: ignore
                    init_method=init_method,  # type: ignore
                    bias=True,
                    input_is_parallel=input_is_parallel,
                    skip_bias_add=False,
                )
            )

        # Activation and regularization
        self.activation = nn.SiLU()
        self.dropout = nn.Dropout(dropout)

        # Output transformation (prevent collapse)
        self.output_scale_raw = nn.Parameter(torch.zeros(1))

        # Initialize weights
        self._reinit_weights()

    def forward(self, hidden_states):
        """Forward pass with batch normalization.

        Args:
            hidden_states: [seq, batch, hidden_size] or [batch, seq, hidden_size]

        Returns:
            output: [seq, batch, 1] or [batch, seq, 1]
            bias: None
        """
        x = hidden_states

        # Determine input format
        if x.shape[0] < x.shape[1]:
            # Likely [seq, batch, hidden] - less common for batch < seq
            seq_first = True
        else:
            # Likely [batch, seq, hidden] - more common
            seq_first = False

        # Apply MLP layers with batch norm
        for i, layer in enumerate(self.layers[:-1]):
            # Linear transformation
            x, _ = layer(x)

            # Batch normalization (if enabled and not final layer)
            if self.use_batch_norm and self.batch_norms is not None:
                # BatchNorm1d expects [batch, channels, length]
                if seq_first:
                    # [seq, batch, hidden] -> [batch, hidden, seq]
                    x = x.permute(1, 2, 0)
                else:
                    # [batch, seq, hidden] -> [batch, hidden, seq]
                    x = x.transpose(1, 2)

                # Apply batch norm
                x = self.batch_norms[i](x)

                # Restore original format
                if seq_first:
                    # [batch, hidden, seq] -> [seq, batch, hidden]
                    x = x.permute(2, 0, 1)
                else:
                    # [batch, hidden, seq] -> [batch, seq, hidden]
                    x = x.transpose(1, 2)

            # Activation and dropout
            x = self.activation(x)
            x = self.dropout(x)

        # Final layer (no batch norm, no activation)
        output, _ = self.layers[-1](x)

        # Ensure positivity
        output = F.elu(output, alpha=0.9) + 1.0

        # Apply learnable scaling (with collapse prevention)
        output_scale = torch.exp(self.output_scale_raw).clamp(min=0.05, max=500.0)

        output = output * output_scale

        return output, None

    def _reinit_weights(self):
        """Initialize weights for stable training."""
        for i, layer in enumerate(self.layers):
            if i == len(self.layers) - 1:  # Final layer
                nn.init.xavier_normal_(layer.weight, gain=0.5)  # type: ignore
                nn.init.constant_(layer.bias, 0.5)  # type: ignore
            else:
                nn.init.xavier_normal_(layer.weight, gain=1.0)  # type: ignore
                # Batch norm has its own bias, so we can init linear bias to zero
                if self.use_batch_norm:
                    nn.init.zeros_(layer.bias)  # type: ignore
                else:
                    nn.init.constant_(layer.bias, 0.1)  # type: ignore
        logging.info(
            f"‚úÖ Initialized {len(self.layers)}-layer BioSignalHead "
            f"{'with' if self.use_batch_norm else 'without'} batch norm"
        )


class ParallelHeadTransform(IOMixin):
    """Parallel Head Transformer.

    Adds RNA expression head to transformer models (e.g., Evo2/Hyena) for multi-task training
    with parallel heads (e.g., DNA and RNA loss).

    This transform allows the model to learn from both DNA sequence modeling and RNA expression
    prediction simultaneously, supporting flexible weighting and toggling of these objectives.

    Args:
        dna_loss_weight (float): Weight assigned to the DNA (language modeling) loss component.
        rna_loss_weight (float): Weight assigned to the RNA expression prediction loss component.
        parallel_dna (bool): Whether to use the DNA head during training/evaluation.
        parallel_rna (bool): Whether to use the RNA head during training/evaluation.

    Inherits from:
        IOMixin - Required by the training framework to enable model saving and serialization.

    Notes:
        - Can be used to add additional predictor heads in future, such as protein expression, CHIPseq, etc.
        - The HyenaModel is modified in-place to add the RNA head and adjust forward logic while respecting the BioNemo
          IOMixin structure.

        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ PyTorch Lightning		        ‚îÇ  ‚Üê Interact with this in predict/training scripts
        ‚îÇ - training_step()		        ‚îÇ
        ‚îÇ - configure_optimizers()      ‚îÇ
        ‚îÇ - Easy API		            ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ NeMo Wrappers		            ‚îÇ  ‚Üê Framework integration
        ‚îÇ - Config management           ‚îÇ
        ‚îÇ - Checkpointing               ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ Megatron-Core                 ‚îÇ  ‚Üê Power under the hood
        ‚îÇ - Tensor Parallelism          ‚îÇ
        ‚îÇ - Pipeline Parallelism        ‚îÇ
        ‚îÇ - Context Parallelism         ‚îÇ
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
        ‚îÇ Core Model                    ‚îÇ  ‚Üê Our actual model, what we modify below
        ‚îÇ - Embedding                   ‚îÇ
        ‚îÇ - Hyena Decoder               ‚îÇ
        ‚îÇ - Output Layer                ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """

    def __init__(
        self,
        dna_loss_weight: float = 1.0,
        rna_loss_weight: float = 1.0,
        pep_loss_weight: float = 1.0,
        parallel_dna: bool = True,
        parallel_rna: bool = True,
        parallel_pep: bool = False,
        predict: bool = False,
        loss_type: Literal["borzoi", "huber", "poisson_dist", "hybrid"] = "hybrid",
        **kwargs,
    ):
        """Initializes the ParallelHeadTransform with specified loss weights and toggles.

        Args:
            dna_loss_weight (float): Weight for DNA language modeling loss.
            rna_loss_weight (float): Weight for RNA expression prediction loss.
            pep_loss_weight (float): Weight for peptide mapping loss.
            parallel_dna (bool): Whether to enable DNA head.
            parallel_rna (bool): Whether to enable RNA head.
            parallel_pep (bool): Whether to enable peptide mapping head.
            predict (bool): Whether the model is in prediction mode.
            loss_type (str): Type of loss function to use for RNA/PEP heads.
            kwargs: Additional keyword arguments for loss functions.
        """
        # Store configurable loss weights and toggles
        self.dna_loss_weight = dna_loss_weight
        self.rna_loss_weight = rna_loss_weight
        self.pep_loss_weight = pep_loss_weight
        self.parallel_dna = parallel_dna
        self.parallel_rna = parallel_rna
        self.parallel_pep = parallel_pep
        self.predict = predict

        # Setup custom loss functions if rna or pep parallelism is enabled
        if self.parallel_rna:
            # Use loss based on user selection
            if loss_type == "borzoi":
                self.rna_loss_fn = BorzoiLoss(**kwargs)
            elif loss_type == "huber":
                self.rna_loss_fn = HuberLoss(**kwargs)
            elif loss_type == "poisson_dist":
                self.rna_loss_fn = PoissonWithDistributionLoss(**kwargs)
            else:
                self.rna_loss_fn = HybridLoss(**kwargs)
        if self.parallel_pep:
            # Use loss based on user selection
            if loss_type == "borzoi":
                self.pep_loss_fn = BorzoiLoss(**kwargs)
            elif loss_type == "huber":
                self.pep_loss_fn = HuberLoss(**kwargs)
            elif loss_type == "poisson_dist":
                self.pep_loss_fn = PoissonWithDistributionLoss(**kwargs)
            else:
                self.pep_loss_fn = HybridLoss(**kwargs)
        # Log model transform initialization
        logging.info(f"üöÄ Parallel Head Transform Initialized with loss type: {loss_type}")

    # ============================================================================
    # Update model to include parallel heads and modified forward logic
    # ============================================================================

    def __call__(self, model: nn.Module) -> nn.Module:
        """Applies the transform to a given model in-place.

        Adds an RNA-seq head if not already present, and modifies the forward pass logic to incorporate both RNA and
        DNA losses (if enabled).

        Args:
            model (nn.Module): The model to be transformed.

        Returns:
            nn.Module: The modified model, augmented with parallel RNA head and updated forward logic.
        """
        # Log current model state for debugging
        debug_heads("Model pre transform", model)

        logging.info("üîß Applying Enhanced ParallelHeadTransform")

        # Extract the main model components
        config = self._get_config(model)
        core_model = self._get_core_hyena_model(model)
        forward_target = self._get_forward_target_model(model)

        # Transform model
        return self._transform_core_model(model, core_model, forward_target, config)

    def _transform_core_model(
        self, model: nn.Module, core_model: nn.Module, forward_target: nn.Module | None, config: HyenaConfig
    ) -> nn.Module:
        """Apply any core model specific transforms.

        Args:
            model (nn.Module): The overall model.
            core_model (nn.Module): The core HyenaModel instance.
            forward_target (nn.Module): The model used during forward pass.
            config (HyenaConfig): The model configuration.

        Returns:
            nn.Module: The transformed model with parallel heads.
        """
        # Use parallel_state to determine if input_is_parallel should be True
        input_is_parallel = parallel_state.is_pipeline_first_stage()

        # Add RNA seq head to core model
        if not hasattr(core_model, "rna_seq_head") and self.parallel_rna:
            # Create RowParallelLinear for RNA-seq head because output is small
            core_model.rna_seq_head = BioSignalHead(
                hidden_size=config.hidden_size,
                intermediate_size=config.hidden_size * 4,
                config=config,
                init_method=config.init_method,
                input_is_parallel=input_is_parallel,
            )

        # Add pep map head to core model
        if not hasattr(core_model, "pep_map_head") and self.parallel_pep:
            # Create RowParallelLinear for pep map head because output is small
            core_model.pep_map_head = BioSignalHead(
                hidden_size=config.hidden_size,
                intermediate_size=config.hidden_size * 4,
                config=config,
                init_method=config.init_method,
                input_is_parallel=input_is_parallel,
            )

        # Set attributes on core model
        core_model.dna_loss_weight = self.dna_loss_weight  # type: ignore
        core_model.rna_loss_weight = self.rna_loss_weight  # type: ignore
        core_model.pep_loss_weight = self.pep_loss_weight  # type: ignore
        core_model.parallel_dna = self.parallel_dna  # type: ignore
        core_model.parallel_rna = self.parallel_rna  # type: ignore
        core_model.parallel_pep = self.parallel_pep  # type: ignore

        # Set up forward target for training
        if forward_target and forward_target != core_model and not self.predict:
            # Copy attributes to forward target
            forward_target.dna_loss_weight = self.dna_loss_weight  # type: ignore
            forward_target.rna_loss_weight = self.rna_loss_weight  # type: ignore
            forward_target.pep_loss_weight = self.pep_loss_weight  # type: ignore
            forward_target.parallel_dna = self.parallel_dna  # type: ignore
            forward_target.parallel_rna = self.parallel_rna  # type: ignore
            forward_target.parallel_pep = self.parallel_pep  # type: ignore
            # Store reference to core model on forward target
            forward_target._core_model = core_model
            if self.parallel_rna:
                forward_target.rna_seq_head = core_model.rna_seq_head
            if self.parallel_pep:
                forward_target.pep_map_head = core_model.pep_map_head

            # Override the forward pass logic with multi-head awareness
            if not hasattr(forward_target, "_original_forward"):
                forward_target._original_forward = forward_target.forward  # type: ignore
                forward_target.forward = self._create_parallel_forward(forward_target, core_model)  # type: ignore

        # Set up forward pass for prediction mode
        # If predict mode, also modify top level model
        if self.predict:
            # Model
            model.dna_loss_weight = self.dna_loss_weight  # type: ignore
            model.rna_loss_weight = self.rna_loss_weight  # type: ignore
            model.pep_loss_weight = self.pep_loss_weight  # type: ignore
            model.parallel_dna = self.parallel_dna  # type: ignore
            model.parallel_rna = self.parallel_rna  # type: ignore
            model.parallel_pep = self.parallel_pep  # type: ignore
            # Overwrite the top-level model forward as well
            if not hasattr(model, "_original_forward"):
                model._original_forward = model.forward  # type: ignore
                model.forward = self._create_parallel_forward(model, core_model)  # type: ignore

        # Log updated model structure
        debug_heads("Model post transform", model)

        # Return the updated model
        return model

    def _get_core_hyena_model(self, model):
        """Get the core HyenaModel from possibly wrapped model."""
        hyena_models = self._discover_all_hyena_models(model)

        # Find the core model
        core_models = [info for info in hyena_models if info["is_core"]]
        if core_models:
            return core_models[0]["model"]

        if hyena_models:
            return hyena_models[0]["model"]

        raise ValueError("‚ùå No HyenaModel found")

    def _get_forward_target_model(self, model: nn.Module) -> Optional[nn.Module]:
        """Get the HyenaModel that will be called during forward step.

        This is typically a wrapper model around the core HyenaModel.

        Args:
            model (nn.Module): The overall model.

        Returns:( nn.Module or None): The model used during forward pass, or None if not found.
        """
        hyena_models = self._discover_all_hyena_models(model)

        # Based on your debug output, the forward step calls the wrapper at level 1
        # Look for wrapper models first
        wrapper_models = [info for info in hyena_models if info["is_wrapper"]]
        if wrapper_models:
            # Usually the first wrapper in the hierarchy is the forward target
            target = wrapper_models[0]
            logging.info(f"‚úÖ Forward target is wrapper at level {target['level']}")
            return target["model"]

        # Fallback to core model
        core_models = [info for info in hyena_models if info["is_core"]]
        if core_models:
            target = core_models[0]
            logging.info(f"‚ö†Ô∏è Using core model as forward target at level {target['level']}")
            return target["model"]

        return None

    def _discover_all_hyena_models(self, model):
        """Unwrap and discover hyena models.

        Find ALL HyenaModel instances in the model hierarchy.

        ### Note
            - Models have a unique ID per GPU in multi-GPU systems.
        """
        hyena_models = []
        current_model = model
        unwrap_count = 0

        logging.info("üîç Discovering HyenaModel instances...")

        while unwrap_count < 6:  # Increased safety limit
            logging.info(f"   Level {unwrap_count}: {type(current_model)} (id: {id(current_model)})")

            # Check if this is ANY kind of HyenaModel
            type_name = type(current_model).__name__
            if "HyenaModel" in type_name:
                model_info = {
                    "model": current_model,
                    "level": unwrap_count,
                    "type": type(current_model),
                    "id": id(current_model),
                    "has_embedding": hasattr(current_model, "embedding"),
                    "has_decoder": hasattr(current_model, "decoder"),
                    "has_output_layer": hasattr(current_model, "output_layer"),
                    "has_forward": hasattr(current_model, "forward"),
                }

                # Determine if this is core or wrapper
                is_core = model_info["has_embedding"] and model_info["has_decoder"] and model_info["has_output_layer"]
                model_info["is_core"] = is_core
                model_info["is_wrapper"] = not is_core

                hyena_models.append(model_info)
                logging.info(f"   ‚úÖ Found {'Core' if is_core else 'Wrapper'} HyenaModel at level {unwrap_count}")
                logging.info(f"      Has core attributes: {is_core}")

            # Try to unwrap further
            if hasattr(current_model, "module"):
                current_model = current_model.module
                unwrap_count += 1
            else:
                logging.info(f"   ‚èπÔ∏è Cannot unwrap further at level {unwrap_count}")
                break

        logging.info(f"üîç Discovery complete: Found {len(hyena_models)} HyenaModel instances")
        for i, info in enumerate(hyena_models):
            logging.info(f"   {i}: {'Core' if info['is_core'] else 'Wrapper'} at level {info['level']}")

        return hyena_models

    def _get_config(self, model) -> HyenaConfig:
        """Get the config from model, trying different attribute names and wrapper levels."""
        # Try transformer_config first (preferred)
        if hasattr(model, "transformer_config"):
            logging.info("‚úÖ Using model.transformer_config")
            return model.transformer_config

        # Try config
        if hasattr(model, "config"):
            logging.info("‚úÖ Using model.config")
            return model.config

        # Try to find config in the hierarchy
        current = model
        level = 0
        while hasattr(current, "module") and level < 5:
            current = current.module
            if hasattr(current, "transformer_config"):
                logging.info(f"‚úÖ Using transformer_config from level {level + 1}")
                return current.transformer_config
            if hasattr(current, "config"):
                logging.info(f"‚úÖ Using config from level {level + 1}")
                return current.config
            level += 1

        raise AttributeError(f"‚ùå Model {type(model)} has no accessible config in hierarchy")

    def _create_parallel_forward(self, target_model: ForwardHyenaModel, core_model: CoreHyenaModel):
        """Creates a custom forward function for a model with parallel DNA and RNA heads.

        This method overrides the default forward pass of the model to support both:
        - Language modeling (DNA) via cross-entropy loss
        - RNA-seq regression via mean squared error loss

        The returned function supports training and inference modes, and dynamically
        adjusts based on config and input availability.

        Args:
            target_model (ForwardHyenaModel): The wrapper model whose `forward` method will be replaced.
            core_model (CoreHyenaModel): The underlying transformer model (e.g., Hyena) used for computation.

        Returns:
            Callable: A function that can be used as a `forward()` method with support for
                      parallel multi-task objectives (DNA and RNA).
        """
        config = self._get_config(target_model)
        LOGGING = True

        def parallel_forward(
            input_ids: torch.Tensor,
            position_ids: None | torch.Tensor = None,
            attention_mask: None | torch.Tensor = None,
            decoder_input: None | torch.Tensor = None,
            labels: None | torch.Tensor = None,
            loss_mask: None | torch.Tensor = None,
            rna_mask: None | torch.Tensor = None,
            pep_mask: None | torch.Tensor = None,
            inference_context: None = None,
            runtime_gather_output: Optional[bool] = None,
            rna_seq_targets: None | torch.Tensor = None,
            pep_map_targets: None | torch.Tensor = None,
            **kwargs,
        ):
            # -------------------------------------------
            # SHARED PREPROCESSING (Embeddings + Decoder)
            # -------------------------------------------

            # Step 1: Token embedding
            if decoder_input is not None:
                # Use pre-computed embedding if provided
                pass
            elif config.pre_process:
                decoder_input = core_model.embedding(input_ids=input_ids, position_ids=position_ids)
            else:
                decoder_input = None  # Skip embedding if pre-process is disabled

            # Step 2: Apply Rotary Embedding (if configured)
            rotary_pos_emb = None
            if config.position_embedding_type == "rope":
                rotary_seq_len = core_model.rotary_pos_emb.get_rotary_seq_len(
                    inference_context,  # type: ignore
                    core_model.decoder,  # type: ignore
                    decoder_input,  # type: ignore
                    config,
                    None,  # type: ignore
                )
                rotary_pos_emb = core_model.rotary_pos_emb(rotary_seq_len)

            # Step 3: Pass through the decoder
            hidden_states = core_model.decoder(
                hidden_states=decoder_input,
                attention_mask=attention_mask,
                rotary_pos_emb=rotary_pos_emb,
            )

            # If model is not post-processing (e.g., encoder-only), return decoder output
            if not config.post_process:
                return hidden_states

            # -------------------------------------------
            # PARALLEL HEAD COMPUTATION (DNA, RNA, etc)
            # -------------------------------------------

            # DNA laguage model head
            logits = None
            if target_model.parallel_dna:
                output_weight = None
                # Shared weights as embedding layer
                if (
                    hasattr(core_model, "share_embeddings_and_output_weights")
                    and core_model.share_embeddings_and_output_weights
                ):
                    output_weight = core_model.shared_embedding_or_output_weight()

                logits, _ = core_model.output_layer(hidden_states, weight=output_weight)

            # RNA seq head
            rna_seq_logits = None
            if target_model.parallel_rna:
                # Processed through core model
                rna_seq_logits, _ = core_model.rna_seq_head(hidden_states)  # type: ignore # Output dim: (batch, seq_len, 1)
                rna_seq_logits = rna_seq_logits.squeeze(-1)

                if LOGGING:
                    # Compute weight norms for logging
                    rna_pred_min = rna_seq_logits.clone().min().item() if rna_seq_logits is not None else 0
                    rna_pred_max = rna_seq_logits.clone().max().item() if rna_seq_logits is not None else 0

            # PEP map head
            pep_map_logits = None
            if target_model.parallel_pep:
                # Processed through core model
                pep_map_logits, _ = core_model.pep_map_head(hidden_states)  # type: ignore
                pep_map_logits = pep_map_logits.squeeze(-1)

            # -------------------------------------------
            # INFERENCE MODE (no labels)
            # -------------------------------------------
            if labels is None:
                inference = {
                    "logits": logits.transpose(0, 1).contiguous() if logits is not None else None,
                    "rna_seq_logits": rna_seq_logits,
                    "pep_map_logits": pep_map_logits,
                }
                return inference

            # -------------------------------------------
            # TRAINING MODE: LOSS COMPUTATION
            # -------------------------------------------
            total_loss = None

            # DNA Language Modeling Loss (cross-entropy)
            if target_model.parallel_dna and labels is not None and logits is not None:
                # Convert labels to handle uppercase/lowercase weighting
                labels, lowercase_mask = make_upper_case(labels)

                # Compute standard LM loss
                loss = core_model.compute_language_model_loss(labels, logits)
                # Normalize or reweight lowercase loss tokens
                normalize_per_batch = config.to_upper == "normalized_weighted"
                loss = reweighted_cross_entropy(
                    loss,
                    (labels, loss_mask, lowercase_mask),
                    lowercase_weight=0.1,
                    normalize_per_batch=normalize_per_batch,
                )

                # Weight and accumulate DNA loss
                loss *= target_model.dna_loss_weight  # type: ignore
                dna_loss = loss.detach().mean()
                total_loss = loss if total_loss is None else total_loss + loss

            # RNA-Seq Loss (Borzoi Loss)
            if target_model.parallel_rna and rna_seq_logits is not None and rna_seq_targets is not None:
                # Ensure matching dtypes
                rna_seq_targets = rna_seq_targets.to(dtype=rna_seq_logits.dtype)

                # Align shape if transposed
                if rna_seq_logits.shape != rna_seq_targets.shape:
                    if (
                        rna_seq_logits.shape[0] == rna_seq_targets.shape[1]
                        and rna_seq_logits.shape[1] == rna_seq_targets.shape[0]
                    ):
                        rna_seq_logits = rna_seq_logits.transpose(0, 1)

                # Compute Borzoi loss
                rna_seq_loss = self.rna_loss_fn.compute(
                    predictions=rna_seq_logits, targets=rna_seq_targets, mask=rna_mask
                )

                # Weight and accumulate rna seq loss
                rna_seq_loss *= target_model.rna_loss_weight  # type: ignore
                rna_loss = rna_seq_loss.detach().mean()
                total_loss = rna_seq_loss if total_loss is None else total_loss + rna_seq_loss

            # PEP-MAP Loss (Borzoi Loss)
            if target_model.parallel_pep and pep_map_logits is not None and pep_map_targets is not None:
                # Ensure matching dtypes
                pep_map_targets = pep_map_targets.to(dtype=pep_map_logits.dtype)

                # Align shape if transposed
                if pep_map_logits.shape != pep_map_targets.shape:
                    if (
                        pep_map_logits.shape[0] == pep_map_targets.shape[1]
                        and pep_map_logits.shape[1] == pep_map_targets.shape[0]
                    ):
                        pep_map_logits = pep_map_logits.transpose(0, 1)

                # Compute MSE loss (element-wise)
                pep_map_loss = self.pep_loss_fn.compute(
                    predictions=pep_map_logits, targets=pep_map_targets, mask=pep_mask
                )

                # Weight and accumulate pep map loss
                pep_map_loss *= target_model.pep_loss_weight  # type: ignore
                pep_loss = pep_map_loss.detach().mean()
                total_loss = pep_map_loss if total_loss is None else total_loss + pep_map_loss

            # Convert loss to correct dtype for model stability (bfloat16)
            if total_loss is not None:
                total_loss = total_loss.to(dtype=torch.bfloat16)

                # Logging individual losses
                if LOGGING:
                    # Compute weight norms for logging
                    rna_target_min = rna_seq_targets.min().item() if rna_seq_targets is not None else 0
                    rna_target_max = rna_seq_targets.max().item() if rna_seq_targets is not None else 0
                    # Format min/max to 2 decimal places
                    rna_target_min = f"{rna_target_min:.2f}"
                    rna_target_max = f"{rna_target_max:.2f}"
                    rna_pred_min = f"{rna_pred_min:.2f}"  # type: ignore
                    rna_pred_max = f"{rna_pred_max:.2f}"  # type: ignore

                    logging.info(
                        f"üßÆ Losses - Total: {total_loss.detach().mean()} | DNA: {dna_loss if 'dna_loss' in locals() else 'N/A'} | RNA: {rna_loss if 'rna_loss' in locals() else 'N/A'} ({rna_pred_min}/{rna_pred_max} vs {rna_target_min}/{rna_target_max}) | PEP: {pep_loss if 'pep_loss' in locals() else 'N/A'}"  # type: ignore
                    )

            return total_loss

        return parallel_forward


def parallel_head_forward_step_fn(model, batch: Dict[str, Any], predict: bool = False) -> torch.Tensor:
    """Executes a forward pass through the model.

    This function is designed to be modular and compatible with models like Hyena
    or GPT-style transformer models that use standard input names (`input_ids`, `position_ids`, etc.),
    while also optionally handling RNA-seq targets if present. Modified to support parallel heads. Including:
        - DNA language modeling
        - RNA expression prediction
        - Peptide mapping prediction

    Args:
        model (torch.nn.Module): The model to be evaluated. Expected to support keyword arguments
                                like `input_ids`, `position_ids`, `attention_mask`, `labels`,
                                `loss_mask`, and optionally `rna_seq_targets`.
        batch (dict): A dictionary of input tensors for the forward pass. Must include:
            - tokens (torch.Tensor): Tokenized input sequences.
            - position_ids (torch.Tensor): Positional encodings.
            - attention_mask (torch.Tensor, optional): Attention mask for padding/masking.
            - labels (torch.Tensor): Ground truth labels for loss calculation.
            - loss_mask (torch.Tensor): Mask to indicate which token losses to consider.
            - rna_seq_targets (torch.Tensor, optional): Additional output targets for RNA prediction tasks.
        predict (bool): Whether the function is being called in prediction mode.

    Returns:
        torch.Tensor: The model's output from the forward pass (e.g., loss or logits).
    """
    # Prepare forward args
    forward_args = {
        "input_ids": batch["tokens"],
        "position_ids": batch["position_ids"],
        "attention_mask": batch.get("attention_mask", None),
        "labels": batch.get("labels", None),
        "loss_mask": batch.get("loss_mask", None),
    }

    # Safely add rna_seq_targets to forward args
    if "rna_seq_targets" in batch:
        forward_args["rna_seq_targets"] = batch.get("rna_seq_targets", None)

    # Safely add pep_map_targets to forward args
    if "pep_map_targets" in batch:
        forward_args["pep_map_targets"] = batch.get("pep_map_targets", None)

    # Safely add rna_mask if present
    if not predict:
        forward_args["rna_mask"] = batch.get("rna_mask", None)

    # Safely add pep_mask if present
    if not predict:
        forward_args["pep_mask"] = batch.get("pep_mask", None)

    # Perform the forward pass using keyword argument unpacking
    result = model(**forward_args)

    # Return model output (typically loss or prediction logits)
    return result


def parallel_head_data_step_fn(
    dataloader_iter, use_mtp: bool = False, predict: bool = False
) -> dict[str, torch.Tensor]:
    """Retrieve and process the next batch of data from a dataloader.

    Inner function used during training steps to retrieve and process the next batch of data from a dataloader. Supports
    modular tensor routing across pipeline stages. This function handles the data loading step for GPT models, managing
    pipeline parallelism by distributing data appropriately across pipeline stages.

    Args:
        dataloader_iter: Iterator over the dataloader
        use_mtp: Whether the Multi-Token Prediction Module is used. Input needs to be passed
                into the last pipeline stage if mtp is used.
        predict: Whether the function is being called in prediction mode.

    Returns:
        dict[str, torch.Tensor]: Processed batch with required tensors moved to appropriate devices

    Note:
        To add additional arguments to forward, make sure to add to `required_device_keys` by making the key value
        exists in `_batch`.
            - For example:
                ```
                if "rna_seq_targets" in _batch:
                    required_device_keys.add("rna_seq_targets")
                ```
    """
    # Based on: https://github.com/NVIDIA/Megatron-LM/blob/main/pretrain_gpt.py#L87
    # https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/language_modeling/megatron_gpt_model.py#L828-L842

    # Fetch the next batch from the dataloader
    batch = next(dataloader_iter)

    # Handle both single-dict and (dict, _, _) tuple formats
    _batch: dict
    if isinstance(batch, tuple) and len(batch) == 3:
        _batch = batch[0]
    else:
        _batch = batch  # type: ignore

    # Sets for determining which keys need to be transferred to which device
    required_device_keys = set()  # Tensors needed on GPU
    required_host_keys = set()  # Tensors needed on CPU

    # Add target keys if they exist
    if "rna_seq_targets" in _batch:
        required_device_keys.add("rna_seq_targets")
    if "pep_map_targets" in batch:
        required_device_keys.add("pep_map_targets")
    # Add masks if they exist
    if "rna_mask" in _batch:
        required_device_keys.add("rna_mask")
    if "pep_mask" in batch:
        required_device_keys.add("pep_mask")

    # cu_seqlens-related values needed for FlashAttention or similar ops
    required_device_keys.add("attention_mask")
    if "cu_seqlens" in _batch:
        required_device_keys.add("cu_seqlens")
        required_host_keys.add("cu_seqlens_argmin")
        required_host_keys.add("max_seqlen")

    # If we're in the first pipeline stage or using MTP, we need input tokens
    if parallel_state.is_pipeline_first_stage() or use_mtp:
        required_device_keys.update(("tokens", "position_ids"))
        # RNA head loss may be computed in the final stage
        if "rna_seq_targets" in _batch:
            required_device_keys.add("rna_seq_targets")  # TODO: Not sure if needed here...
        if "pep_map_targets" in batch:
            required_device_keys.add("pep_map_targets")  # TODO: Not sure if needed here...
        # Also add masks if they exist
        if "rna_mask" in _batch:
            required_device_keys.add("rna_mask")  # TODO: Not sure if needed here...
        if "pep_mask" in batch:
            required_device_keys.add("pep_mask")  # TODO: Not sure if needed here...

    # If we're in the last pipeline stage, we need output labels for loss computation
    if parallel_state.is_pipeline_last_stage():
        required_device_keys.update(("labels", "loss_mask", "seq_idx") if predict else ("labels", "loss_mask"))
        # RNA head loss may be computed in the final stage
        if "rna_seq_targets" in _batch:
            required_device_keys.add("rna_seq_targets")
        if "pep_map_targets" in batch:
            required_device_keys.add("pep_map_targets")
        # Also add masks if they exist
        if "rna_mask" in _batch:
            required_device_keys.add("rna_mask")
        if "pep_mask" in batch:
            required_device_keys.add("pep_mask")

    # Dictionary that will hold the appropriately placed tensors (CPU/GPU/None)
    _batch_required_keys = {}
    for key, val in _batch.items():
        if key in required_device_keys:
            _batch_required_keys[key] = val.cuda(non_blocking=True)
        elif key in required_host_keys:
            _batch_required_keys[key] = val.cpu()
        else:
            _batch_required_keys[key] = None

    # slice batch along sequence dimension for context parallelism
    output = get_batch_on_this_cp_rank(_batch_required_keys)

    return output
